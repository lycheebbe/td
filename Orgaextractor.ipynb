{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lycheebbe/td/blob/main/Orgaextractor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RflcvX_AT1EP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7df32aa7-91ce-41f4-82f2-b6da173f5f7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: medpy in /usr/local/lib/python3.11/dist-packages (0.5.2)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.11/dist-packages (from medpy) (1.15.3)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from medpy) (2.0.2)\n",
            "Requirement already satisfied: SimpleITK>=2.1 in /usr/local/lib/python3.11/dist-packages (from medpy) (2.5.2)\n"
          ]
        }
      ],
      "source": [
        "## requirements\n",
        "import argparse\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import random\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision import transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm, trange\n",
        "!pip install medpy\n",
        "import medpy.metric.binary as bin\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PTtEnsRN9N_a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd8dea1f-3807-4474-8498-f38e47dc6aa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1wOzvgroIgpEA9kaYfbz0Q3vUL5GY1my9&confirm=t\n",
            "To: /content/orgaextractor.pth\n",
            "100% 1.08G/1.08G [00:14<00:00, 76.5MB/s]\n",
            "mkdir: cannot create directory ‘result’: File exists\n",
            "mkdir: cannot create directory ‘test’: File exists\n"
          ]
        }
      ],
      "source": [
        "## We first want to load our dataset and convert it to numpy\n",
        "\"\"\"\n",
        "data file has to be in image format such as jpeg, png etc..\n",
        "Not ready for numpy file yet.\n",
        "A User only needs to change their dataset path in local and set result path.\n",
        "YOU MUST EITHER DOWNLOAD OUR TEST DATA OR HAVE YOUR OWN.\n",
        "\"\"\"\n",
        "\n",
        "!gdown \"1wOzvgroIgpEA9kaYfbz0Q3vUL5GY1my9&confirm=t\" # Model weight file, takes about 20 secs, file will be stored under content\n",
        "!mkdir result\n",
        "!mkdir test\n",
        "data_path = '/content/test'\n",
        "result_dir = '/content/result'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GqNCCH5-Ufbh"
      },
      "outputs": [],
      "source": [
        "## implement model\n",
        "class Residual_block_3(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(Residual_block_3, self).__init__()\n",
        "        layers = []\n",
        "        layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                                kernel_size=3, stride=1, padding=1,\n",
        "                                bias=True)]\n",
        "        layers += [nn.InstanceNorm2d(num_features=out_channels)]\n",
        "        layers += [nn.ReLU()]\n",
        "        layers += [nn.Conv2d(in_channels=out_channels, out_channels=out_channels,\n",
        "                                kernel_size=3, stride=1, padding=1,\n",
        "                                bias=True)]\n",
        "        layers += [nn.InstanceNorm2d(num_features=out_channels)]\n",
        "        layers += [nn.ReLU()]\n",
        "\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "\n",
        "        skips = []\n",
        "        skips += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                            kernel_size=3, stride=1, padding=1,\n",
        "                            bias=True)]\n",
        "        skips += [nn.InstanceNorm2d(num_features=out_channels)]\n",
        "\n",
        "        self.skip = nn.Sequential(*skips)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x) + self.skip(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Residual_block_7(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(Residual_block_7, self).__init__()\n",
        "        layers = []\n",
        "        layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                                kernel_size=7, stride=1, padding=3,\n",
        "                                bias=True)]\n",
        "        layers += [nn.InstanceNorm2d(num_features=out_channels)]\n",
        "        layers += [nn.ReLU()]\n",
        "        layers += [nn.Conv2d(in_channels=out_channels, out_channels=out_channels,\n",
        "                                kernel_size=7, stride=1, padding=3,\n",
        "                                bias=True)]\n",
        "        layers += [nn.InstanceNorm2d(num_features=out_channels)]\n",
        "        layers += [nn.ReLU()]\n",
        "\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "\n",
        "        skips = []\n",
        "        skips += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                            kernel_size=7, stride=1, padding=3,\n",
        "                            bias=True)]\n",
        "        skips += [nn.InstanceNorm2d(num_features=out_channels)]\n",
        "\n",
        "        self.skip = nn.Sequential(*skips)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x) + self.skip(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Residual_block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(Residual_block, self).__init__()\n",
        "        self.x3 = Residual_block_3(in_channels, out_channels)\n",
        "        self.x7 = Residual_block_7(in_channels, out_channels)\n",
        "\n",
        "        self.conv = nn.Conv2d(out_channels * 2, out_channels, kernel_size=1, stride=1, padding=0, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x3 = self.x3(x)\n",
        "        x7 = self.x7(x)\n",
        "\n",
        "        x = torch.cat((x3, x7), dim=1)\n",
        "        x = self.conv(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class ResUNet_MS(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResUNet_MS, self).__init__()\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc1_1 = Residual_block(in_channels=1, out_channels=64)\n",
        "\n",
        "        self.enc2_1 = Residual_block(in_channels=64, out_channels=128)\n",
        "\n",
        "        self.enc3_1 = Residual_block(in_channels=128, out_channels=256)\n",
        "\n",
        "        self.enc4_1 = Residual_block(in_channels=256, out_channels=512)\n",
        "\n",
        "        self.enc5_1 = Residual_block(in_channels=512, out_channels=1024)\n",
        "\n",
        "        self.unpool5 = nn.ConvTranspose2d(in_channels=1024, out_channels=512,\n",
        "                                        kernel_size=2, stride=2, padding=0, bias=True)\n",
        "        self.dec5_1 = Residual_block(in_channels=1024, out_channels=512)\n",
        "\n",
        "        self.unpool4 = nn.ConvTranspose2d(in_channels=512, out_channels=256,\n",
        "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
        "        self.dec4_1 = Residual_block(in_channels=512, out_channels=256)\n",
        "\n",
        "        self.unpool3 = nn.ConvTranspose2d(in_channels=256, out_channels=128,\n",
        "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
        "        self.dec3_1 = Residual_block(in_channels=256, out_channels=128)\n",
        "\n",
        "        self.unpool2 = nn.ConvTranspose2d(in_channels=128, out_channels=64,\n",
        "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
        "        self.dec2_1 = Residual_block(in_channels=128, out_channels=64)\n",
        "\n",
        "\n",
        "        self.fc = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1, stride=1, padding=0, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc1_1 = self.enc1_1(x)\n",
        "\n",
        "        pool2 = self.pool(enc1_1)\n",
        "        enc2_1 = self.enc2_1(pool2)\n",
        "\n",
        "        pool3 = self.pool(enc2_1)\n",
        "        enc3_1 = self.enc3_1(pool3)\n",
        "\n",
        "        pool4 = self.pool(enc3_1)\n",
        "        enc4_1 = self.enc4_1(pool4)\n",
        "\n",
        "        pool5 = self.pool(enc4_1)\n",
        "        enc5_1 = self.enc5_1(pool5)\n",
        "\n",
        "        unpool5 = self.unpool5(enc5_1)\n",
        "        cat5 = torch.cat((unpool5, enc4_1), dim=1)\n",
        "        dec5_1 = self.dec5_1(cat5)\n",
        "\n",
        "        unpool4 = self.unpool4(dec5_1)\n",
        "        cat4 = torch.cat((unpool4, enc3_1), dim=1)\n",
        "        dec4_1 = self.dec4_1(cat4)\n",
        "\n",
        "        unpool3 = self.unpool3(dec4_1)\n",
        "        cat3 = torch.cat((unpool3, enc2_1), dim=1)\n",
        "        dec3_1 = self.dec3_1(cat3)\n",
        "\n",
        "        unpool2 = self.unpool2(dec3_1)\n",
        "        cat2 = torch.cat((unpool2, enc1_1), dim=1)\n",
        "        dec2_1 = self.dec2_1(cat2)\n",
        "\n",
        "        x = self.fc(dec2_1)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bli\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def center_crop_to_match(tensor_to_crop, reference_tensor):\n",
        "    \"\"\"Crops tensor_to_crop to match the spatial size of reference_tensor.\"\"\"\n",
        "    _, _, h1, w1 = reference_tensor.size()\n",
        "    _, _, h2, w2 = tensor_to_crop.size()\n",
        "    crop_h = (h2 - h1) // 2\n",
        "    crop_w = (w2 - w1) // 2\n",
        "    return tensor_to_crop[:, :, crop_h:crop_h + h1, crop_w:crop_w + w1]\n",
        "\n",
        "# Patch forward function in your existing model class\n",
        "def patched_forward(self, x):\n",
        "    enc1_1 = self.enc1_1(x)\n",
        "    pool2 = self.pool(enc1_1)\n",
        "    enc2_1 = self.enc2_1(pool2)\n",
        "\n",
        "    pool3 = self.pool(enc2_1)\n",
        "    enc3_1 = self.enc3_1(pool3)\n",
        "\n",
        "    pool4 = self.pool(enc3_1)\n",
        "    enc4_1 = self.enc4_1(pool4)\n",
        "\n",
        "    pool5 = self.pool(enc4_1)\n",
        "    enc5_1 = self.enc5_1(pool5)\n",
        "\n",
        "    unpool5 = self.unpool5(enc5_1)\n",
        "    enc4_1 = center_crop_to_match(enc4_1, unpool5)\n",
        "    cat5 = torch.cat((unpool5, enc4_1), dim=1)\n",
        "    dec5_1 = self.dec5_1(cat5)\n",
        "\n",
        "    unpool4 = self.unpool4(dec5_1)\n",
        "    enc3_1 = center_crop_to_match(enc3_1, unpool4)\n",
        "    cat4 = torch.cat((unpool4, enc3_1), dim=1)\n",
        "    dec4_1 = self.dec4_1(cat4)\n",
        "\n",
        "    unpool3 = self.unpool3(dec4_1)\n",
        "    enc2_1 = center_crop_to_match(enc2_1, unpool3)\n",
        "    cat3 = torch.cat((unpool3, enc2_1), dim=1)\n",
        "    dec3_1 = self.dec3_1(cat3)\n",
        "\n",
        "    unpool2 = self.unpool2(dec3_1)\n",
        "    enc1_1 = center_crop_to_match(enc1_1, unpool2)\n",
        "    cat2 = torch.cat((unpool2, enc1_1), dim=1)\n",
        "    dec2_1 = self.dec2_1(cat2)\n",
        "\n",
        "    x = self.fc(dec2_1)\n",
        "    return x\n",
        "\n",
        "# Apply patch\n",
        "ResUNet_MS.forward = patched_forward\n"
      ],
      "metadata": {
        "id": "ST-P3e7QodYF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a helper function"
      ],
      "metadata": {
        "id": "5eOB2xYDmFCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def center_crop_or_pad(img, target_shape=(1600, 1100)):\n",
        "    h, w = img.shape[:2]\n",
        "    target_h, target_w = target_shape\n",
        "\n",
        "    pad_h = max(target_h - h, 0)\n",
        "    pad_w = max(target_w - w, 0)\n",
        "\n",
        "    # Pad if too small\n",
        "    if pad_h > 0 or pad_w > 0:\n",
        "        top = pad_h // 2\n",
        "        bottom = pad_h - top\n",
        "        left = pad_w // 2\n",
        "        right = pad_w - left\n",
        "        img = np.pad(img, ((top, bottom), (left, right), (0, 0)), mode='constant', constant_values=0)\n",
        "\n",
        "    # Crop if too big\n",
        "    h, w = img.shape[:2]\n",
        "    start_y = (h - target_h) // 2\n",
        "    start_x = (w - target_w) // 2\n",
        "    img = img[start_y:start_y + target_h, start_x:start_x + target_w, :]\n",
        "\n",
        "    return img"
      ],
      "metadata": {
        "id": "aNpb6HxdmEKb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xf9SbZB2Vj_h"
      },
      "outputs": [],
      "source": [
        "## we would have to transform data if image size is too large\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_dir):\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        lst_data = os.listdir(self.data_dir)\n",
        "\n",
        "        lst_input = [f for f in lst_data if f.startswith('input')]\n",
        "\n",
        "        lst_input.sort()\n",
        "\n",
        "        self.lst_input = lst_input\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lst_input)\n",
        "\n",
        "    # for test\n",
        "    def test_transform(self, image):\n",
        "        # Transform to tensor\n",
        "        image = TF.to_tensor(image)\n",
        "\n",
        "        image = TF.normalize(image, 0.5, 0.5)\n",
        "\n",
        "        return image\n",
        "\n",
        "    # Original Code\n",
        "    # def __getitem__(self, index):\n",
        "\n",
        "    #     p = os.path.join(self.data_dir, self.lst_input[index])\n",
        "\n",
        "    #     if p.endswith('npy'):\n",
        "    #       input = np.load(p)\n",
        "    #     else:\n",
        "    #       input = cv2.imread(os.path.join(self.data_dir, self.lst_input[index]), 0)\n",
        "\n",
        "    #     input = input/255.0\n",
        "\n",
        "    #     if input.ndim == 2:\n",
        "    #         input = input[:, :, np.newaxis]\n",
        "\n",
        "    #     # resize\n",
        "    #     # Original Code\n",
        "    #     # input = cv2.resize(input, (512,512), interpolation=cv2.INTER_AREA)\n",
        "    #     input = cv2.resize(input, (1100,1600), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    #     input = self.test_transform(input)\n",
        "\n",
        "    #     return input\n",
        "\n",
        "    # bli\n",
        "    def __getitem__(self, index):\n",
        "        import torchvision.transforms.functional as TF\n",
        "\n",
        "        p = os.path.join(self.data_dir, self.lst_input[index])\n",
        "\n",
        "        if p.endswith('.npy'):\n",
        "            input = np.load(p)\n",
        "        else:\n",
        "            input = cv2.imread(p, 0)  # Read as grayscale\n",
        "\n",
        "        input = input / 255.0\n",
        "\n",
        "        if input.ndim == 2:\n",
        "            input = input[:, :, np.newaxis]\n",
        "\n",
        "        # 🔧 Make sure input has shape (1600, 1100)\n",
        "        input = center_crop_or_pad(input, target_shape=(1600, 1100))\n",
        "\n",
        "        # Convert to tensor & normalize\n",
        "        input = TF.to_tensor(input)\n",
        "        input = TF.normalize(input, 0.5, 0.5)\n",
        "\n",
        "        return input\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dRBv95mxbNWy"
      },
      "outputs": [],
      "source": [
        "## pp\n",
        "def draw_contour(args: np.ndarray):\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7,7))\n",
        "\n",
        "    pred= args\n",
        "\n",
        "    pred = pred // 255\n",
        "\n",
        "    # erase metric bar\n",
        "    pred[1080:, 1400:] = 0\n",
        "\n",
        "    o = np.uint8(pred)\n",
        "    contours, hie= cv2.findContours(o, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "\n",
        "    r = cv2.fillPoly(o, pts=contours, color=(255,255,255))\n",
        "\n",
        "    o = cv2.morphologyEx(r, cv2.MORPH_OPEN, kernel, iterations=2)\n",
        "\n",
        "    pp = o\n",
        "\n",
        "    o = np.uint8(o//255)\n",
        "\n",
        "    contours, hie= cv2.findContours(o, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    img_contour = cv2.drawContours(o, contours, -1, color=(255, 255, 255), thickness=5)\n",
        "\n",
        "    return img_contour, contours, hie, pp\n",
        "\n",
        "\n",
        "\n",
        "def analysis(img_contour, contours, hie):\n",
        "    info = {}\n",
        "    c = contours\n",
        "    c_im = img_contour\n",
        "    for i, x in enumerate(c):\n",
        "        tmp = {}\n",
        "        M = cv2.moments(x)\n",
        "\n",
        "        area = M['m00']\n",
        "        if area == 0.0:\n",
        "            continue\n",
        "\n",
        "        cX = int(M['m10'] / M['m00'])\n",
        "        cY = int(M['m01'] / M['m00'])\n",
        "\n",
        "\n",
        "        _,radius = cv2.minEnclosingCircle(x)\n",
        "        _, (minorAxisLength, majorAxisLength), angle = cv2.fitEllipse(x)\n",
        "\n",
        "        a = majorAxisLength / 2\n",
        "        b = minorAxisLength / 2\n",
        "\n",
        "        Eccentricity = round(np.sqrt(pow(a, 2) - pow(b, 2))/a, 2)\n",
        "\n",
        "        radius = int(radius)\n",
        "        diameter_in_pixels = radius * 2\n",
        "\n",
        "        cv2.putText(c_im, text=str(i+1), org=(cX, cY), fontFace= cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.5, color=(255,255,255),\n",
        "                thickness=1, lineType=cv2.LINE_AA)\n",
        "        tmp[\"Area\"] = area\n",
        "        tmp[\"Diameter\"] = diameter_in_pixels\n",
        "        tmp[\"majorAxisLength\"] = np.round(majorAxisLength, 2)\n",
        "        tmp[\"minorAxisLength\"] = np.round(minorAxisLength,2)\n",
        "        tmp[\"Eccentricity\"] = Eccentricity\n",
        "        tmp[\"Perimeter\"] = np.round(cv2.arcLength(x, True),2)\n",
        "        info[i+1] = tmp\n",
        "\n",
        "\n",
        "    return info, c_im"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "McT3j-3MT-0V"
      },
      "outputs": [],
      "source": [
        "## define parameters\n",
        "## pretrained_model = final_model.pth\n",
        "# pretrained_model = torch.load('/content/drive/Shareddrives/오가노이드_AI_영상/organoid/chk/final_model.pth')\n",
        "\n",
        "device = 'cuda'\n",
        "pretrained_model = torch.load('/content/orgaextractor.pth')\n",
        "\n",
        "model = ResUNet_MS().to(device)\n",
        "model = nn.DataParallel(module=model).to(device)\n",
        "model.load_state_dict(pretrained_model)\n",
        "# model.load_state_dict(pretrained_model['optim'], strict=False)\n",
        "\n",
        "batch_size = 2\n",
        "\n",
        "fn_tonumpy = lambda x: x.to('cpu').detach().numpy().transpose(0, 2, 3, 1)\n",
        "fn_denorm = lambda x, mean, std: (x * std) + mean\n",
        "fn_class = lambda x: 1.0 * (x > 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PlkBwBVLaLsb"
      },
      "outputs": [],
      "source": [
        "## dataloader\n",
        "dataset_test = Dataset(data_dir=data_path)\n",
        "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
        "num_data_test = len(dataset_test)\n",
        "num_batch_test = np.ceil(num_data_test / batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "E90F9XUsakw4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "8d62fba3-5e34-44e0-8c3e-2b633e1cc19f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-14-3657661458.py:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  amp_grad_scaler = GradScaler()\n",
            "/tmp/ipython-input-14-3657661458.py:23: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'OpenpyxlWriter' object has no attribute 'save'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-14-3657661458.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m               \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'numpy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'input_{id}.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m               \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'numpy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'output_{id}.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m           \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Image saved at: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'OpenpyxlWriter' object has no attribute 'save'"
          ]
        }
      ],
      "source": [
        "## inference\n",
        "\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "amp_grad_scaler = GradScaler()\n",
        "\n",
        "# create result folder if not exists\n",
        "if not os.path.exists(os.path.join(result_dir, 'png')):\n",
        "        os.mkdir(os.path.join(result_dir, 'png'))\n",
        "        os.mkdir(os.path.join(result_dir, 'numpy'))\n",
        "\n",
        "# Setting Excel writer\n",
        "path = os.path.join(result_dir, 'analysis.xlsx')\n",
        "writer = pd.ExcelWriter(path, engine = 'openpyxl')\n",
        "\n",
        "with torch.no_grad():\n",
        "      model.eval()\n",
        "      # loss_arr = []\n",
        "      for batch, data in enumerate(loader_test, 1):\n",
        "      # for data in loader_test:\n",
        "          input = data.to(device, dtype=torch.float)\n",
        "          # label = data[1].to(device, dtype=torch.float)\n",
        "\n",
        "          with autocast():\n",
        "            output = model(input)\n",
        "\n",
        "          # label = fn_tonumpy(label)\n",
        "          input = fn_tonumpy(fn_denorm(input, mean=0.5, std=0.5))\n",
        "          output = fn_tonumpy(fn_class(output))\n",
        "\n",
        "          for j in range(input.shape[0]):\n",
        "\n",
        "              id = batch_size * (batch - 1) + j\n",
        "\n",
        "              # plt.imsave(os.path.join(result_dir, 'png', f'label_{id}.png'), label[j].squeeze(), cmap='gray')\n",
        "              plt.imsave(os.path.join(result_dir, 'png', f'input_{id}.png'), input[j].squeeze(), cmap='gray')\n",
        "              plt.imsave(os.path.join(result_dir, 'png', f'output_{id}.png'), output[j].squeeze(), cmap='gray')\n",
        "\n",
        "              # reread output due to cv2 type\n",
        "              o = os.path.join(result_dir, 'png', f'output_{id}.png')\n",
        "              o = cv2.imread(o, 0)\n",
        "              img_contour, contour, hie, pp = draw_contour(o)\n",
        "              info, c_im = analysis(img_contour, contour, hie)\n",
        "              df = pd.DataFrame(info)\n",
        "              df_t = df.transpose()\n",
        "              df_t.to_excel(writer, sheet_name=f'contour_{id}')\n",
        "              # print(c_im.shape)\n",
        "              plt.imsave(os.path.join(result_dir, 'png', f'contour_{id}.png'), img_contour, cmap='gray')\n",
        "              plt.imsave(os.path.join(result_dir, 'png', f'pp_{id}.png'), pp, cmap='gray')\n",
        "\n",
        "              # np.save(os.path.join(result_dir, 'numpy', f'label_{id}.npy'), label[j].squeeze())\n",
        "              np.save(os.path.join(result_dir, 'numpy', f'input_{id}.npy'), input[j].squeeze())\n",
        "              np.save(os.path.join(result_dir, 'numpy', f'output_{id}.npy'), output[j].squeeze())\n",
        "          writer.save()\n",
        "\n",
        "print('Image saved at: ', os.path.join(result_dir, 'png'))\n",
        "print('Numpy file saved at: ', os.path.join(result_dir, 'numpy'))\n",
        "print('--------------Orgaextractor--------------')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}